{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3edb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use two packages to open Excel files\n",
    "#!pip install xlrd --break-system-packages\n",
    "\n",
    "!pip install openpyxl --break-system-packages\n",
    "\n",
    "# Install the disambiguation package (no need for a master list of firm names)\n",
    "#pip install disamby --break-system-packages\n",
    "!pip install rapidfuzz\n",
    "\n",
    "# Load package for data manipulation purposes\n",
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import fuzz, process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad315306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in June 2015 data to make a complete set of data; get rid of associations that list condo in name and those with zero listed units\n",
    "\n",
    "june15_df = pd.read_excel(\"../Data/NRED/2015/NRED HOA - 06 2015.xls\")\n",
    "june15_df = june15_df[~june15_df['Name'].str.contains('condo', case=False, na=False)]\n",
    "june15_df = june15_df[~(june15_df['# of Units'] == 0)]\n",
    "\n",
    "june15_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca298f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a subset of observations that have management companies associated with them \n",
    "# Then sort in ascending alphabetical order according to the management company names\n",
    "june15_subset = june15_df[june15_df['Address1'].str.contains('C/O', na=False)].sort_values(by='Address1')\n",
    "june15_subset['City'] = june15_subset['City'].str.title()\n",
    "june15_subset['Address1'] = june15_subset['Address1'].str.replace(r'\\bC/O\\b', '', regex=True).str.title()\n",
    "june15_subset['Address2'] = june15_subset['Address2'].str.title()\n",
    "june15_subset['Name'] = june15_subset['Name'].str.title()\n",
    "\n",
    "june15_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kept this the way it was (old, less accurate fuzzy) - see looping for correct code\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def singularize_word(word):\n",
    "    if len(word) > 3 and word.endswith('s') and not word.endswith('ss'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def normalize_address(address):\n",
    "    if pd.isnull(address):\n",
    "        return ''\n",
    "    address = address.lower()\n",
    "    \n",
    "    # Normalize common road types\n",
    "    address = re.sub(r'\\b(streets|street|st)\\b', 'st', address)\n",
    "    address = re.sub(r'\\b(roads|road|rd)\\b', 'rd', address)\n",
    "    address = re.sub(r'\\b(avenues|avenue|ave)\\b', 'ave', address)\n",
    "    address = re.sub(r'\\b(boulevards|boulevard|blvd)\\b', 'blvd', address)\n",
    "    address = re.sub(r'\\b(suites|suite|ste|se)\\b', 'ste', address) \n",
    "    address = re.sub(r'\\b(apartments|apartment|apt)\\b', 'apt', address)\n",
    "    address = re.sub(r'\\b(forts|fort|ft)\\b', 'ft', address)\n",
    "    \n",
    "    # Standardize street directions\n",
    "    address = re.sub(r'\\b(south|s)\\b', 's', address)\n",
    "    address = re.sub(r'\\b(north|n)\\b', 'n', address)\n",
    "    address = re.sub(r'\\b(east|e)\\b', 'e', address)\n",
    "    address = re.sub(r'\\b(west|w)\\b', 'w', address)\n",
    "    address = re.sub(r'\\b(southeast|se)\\b', 'se', address)\n",
    "    address = re.sub(r'\\b(northeast|ne)\\b', 'ne', address)\n",
    "    address = re.sub(r'\\b(southwest|sw)\\b', 'sw', address)\n",
    "    address = re.sub(r'\\b(northwest|nw)\\b', 'nw', address)\n",
    "    \n",
    "    # Eliminate firm office indicators\n",
    "    address = re.split(r'\\b(suite|ste|apt|apartment|unit|site)\\b', address)[0]\n",
    "    \n",
    "    # Remove punctuation and number sign\n",
    "    address = re.sub(r'[.,]', '', address)\n",
    "    address = re.sub(r'\\s+', ' ', address).strip()\n",
    "    address = re.sub(r'#', '', address)\n",
    "\n",
    "    # Singularize words\n",
    "    address = ' '.join(singularize_word(word) for word in address.split())\n",
    "\n",
    "    return address\n",
    "\n",
    "# Do a similar process for the firm names\n",
    "def normalize_firm_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return ''\n",
    "    name = name.lower().strip()\n",
    "    \n",
    "    # Remove suffixes\n",
    "    name = re.sub(r'\\b(llc|inc|corp|co|ltd)\\b', '', name)\n",
    "    \n",
    "    # Remove commas etc.\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    \n",
    "    # Singularize words\n",
    "    name = ' '.join(singularize_word(word) for word in name.split())\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Normalize the phone\n",
    "def normalize_phone(phone):\n",
    "    if pd.isnull(phone):\n",
    "        return ''\n",
    "    return re.sub(r'\\D', '', str(phone))\n",
    "\n",
    "def get_mode(series):\n",
    "    mode = series.mode()\n",
    "    return mode.iloc[0] if not mode.empty else series.iloc[0]\n",
    "\n",
    "june15_subset['Address2_Normalized'] = june15_subset['Address2'].apply(normalize_address)\n",
    "june15_subset['Firm_Normalized'] = june15_subset['Address1'].apply(normalize_firm_name)\n",
    "june15_subset['Phone_Normalized'] = june15_subset['Telephone'].apply(normalize_phone)\n",
    "\n",
    "june15_subset['Firm_Normalized'] = june15_subset['Firm_Normalized'].str.replace(\n",
    "    r'\\bmangement\\b', 'management', regex=True\n",
    ")\n",
    "\n",
    "# Most common phone and address by firm name\n",
    "canonical_phone = (\n",
    "    june15_subset.groupby('Phone_Normalized')\n",
    "    .agg({'Firm_Normalized': get_mode})\n",
    "    .rename(columns={'Firm_Normalized': 'Firm_ByPhone'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "canonical_address = (\n",
    "    june15_subset.groupby('Address2_Normalized')\n",
    "    .agg({'Firm_Normalized': get_mode})\n",
    "    .rename(columns={'Firm_Normalized': 'Firm_ByAddress'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "june15_subset = june15_subset.merge(canonical_phone, on='Phone_Normalized', how='left')\n",
    "june15_subset = june15_subset.merge(canonical_address, on='Address2_Normalized', how='left')\n",
    "\n",
    "# Build the hierarchy\n",
    "def pick_canonical(row):\n",
    "    if pd.notnull(row['Firm_ByPhone']):\n",
    "        return row['Firm_ByPhone']\n",
    "    elif pd.notnull(row['Firm_ByAddress']):\n",
    "        return row['Firm_ByAddress']\n",
    "    else:\n",
    "        return row['Firm_Normalized']\n",
    "\n",
    "june15_subset['Firm_Canonical'] = june15_subset.apply(pick_canonical, axis=1)\n",
    "\n",
    "# Fuzzy matching within the canonical deduplication\n",
    "def fuzzy_within_group(group, threshold=90):\n",
    "    unique_names = group['Firm_Canonical'].unique()\n",
    "    name_map = {}\n",
    "    for name in unique_names:\n",
    "        if name in name_map:\n",
    "            continue\n",
    "        matches = process.extract(name, unique_names, scorer=fuzz.token_set_ratio, limit=None)\n",
    "        close_matches = [m[0] for m in matches if m[1] >= threshold]\n",
    "        for cm in close_matches:\n",
    "            name_map[cm] = name\n",
    "    group['Firm_Final'] = group['Firm_Canonical'].map(name_map).fillna(group['Firm_Canonical'])\n",
    "    return group\n",
    "\n",
    "# Apply fuzzy matching **within each phone group first** to avoid cross-company merges\n",
    "june15_subset = june15_subset.groupby('Phone_Normalized').apply(fuzzy_within_group).reset_index(drop=True)\n",
    "\n",
    "# Fuzzy across all names to catch edge cases\n",
    "all_names = june15_subset['Firm_Final'].unique()\n",
    "name_map_global = {}\n",
    "\n",
    "for name in all_names:\n",
    "    if name in name_map_global:\n",
    "        continue\n",
    "    matches = process.extract(name, all_names, scorer=fuzz.token_set_ratio, limit=None)\n",
    "    close_matches = [m[0] for m in matches if m[1] >= 90]  \n",
    "    for cm in close_matches:\n",
    "        name_map_global[cm] = name\n",
    "\n",
    "june15_subset['Firm_Final'] = june15_subset['Firm_Final'].map(name_map_global).fillna(june15_subset['Firm_Final'])\n",
    "\n",
    "# Save data\n",
    "june15_subset.to_csv('../Data/Cleaned files/june15_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in June 2025 data to make a complete set of data; get rid of associations that list condo in name and zero units cases\n",
    "\n",
    "june25_df = pd.read_excel(\"../Data/NRED/2025/NRED HOA - 06 2025.xlsx\")\n",
    "june25_df = june25_df[~june25_df['Name'].str.contains('condo', case=False, na=False)]\n",
    "june25_df = june25_df[~(june25_df['# of Units'] == 0)]\n",
    "\n",
    "june25_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16588f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another subset\n",
    "june25_subset = june25_df[june25_df['Address1'].str.contains('C/O', na=False)].sort_values(by='Address1')\n",
    "june25_subset['City'] = june25_subset['City'].str.title()\n",
    "june25_subset['Address1'] = june25_subset['Address1'].str.replace(r'\\bC/O\\b', '', regex=True).str.title()\n",
    "june25_subset['Address2'] = june25_subset['Address2'].str.title()\n",
    "june25_subset['Name'] = june25_subset['Name'].str.title()\n",
    "\n",
    "june25_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a50457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat process for second static period\n",
    "# Kept this the way it was (old, less accurate fuzzy) - see looping for correct code\n",
    "\n",
    "june25_subset['Address2_Normalized'] = june25_subset['Address2'].apply(normalize_address)\n",
    "june25_subset['Firm_Normalized'] = june25_subset['Address1'].apply(normalize_firm_name)\n",
    "june25_subset['Phone_Normalized'] = june25_subset['Telephone'].apply(normalize_phone)\n",
    "\n",
    "june25_subset['Firm_Normalized'] = june25_subset['Firm_Normalized'].str.replace(\n",
    "    r'\\bmangement\\b', 'management', regex=True\n",
    ")\n",
    "\n",
    "canonical_phone = (\n",
    "    june25_subset.groupby('Phone_Normalized')\n",
    "    .agg({'Firm_Normalized': get_mode})\n",
    "    .rename(columns={'Firm_Normalized': 'Firm_ByPhone'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "canonical_address = (\n",
    "    june25_subset.groupby('Address2_Normalized')\n",
    "    .agg({'Firm_Normalized': get_mode})\n",
    "    .rename(columns={'Firm_Normalized': 'Firm_ByAddress'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "june25_subset = june25_subset.merge(canonical_phone, on='Phone_Normalized', how='left')\n",
    "june25_subset = june25_subset.merge(canonical_address, on='Address2_Normalized', how='left')\n",
    "\n",
    "june25_subset['Firm_Canonical'] = june25_subset.apply(pick_canonical, axis=1)\n",
    "\n",
    "june25_subset = june25_subset.groupby('Phone_Normalized').apply(fuzzy_within_group).reset_index(drop=True)\n",
    "\n",
    "all_names = june25_subset['Firm_Final'].unique()\n",
    "name_map_global = {}\n",
    "\n",
    "for name in all_names:\n",
    "    if name in name_map_global:\n",
    "        continue\n",
    "    matches = process.extract(name, all_names, scorer=fuzz.token_set_ratio, limit=None)\n",
    "    close_matches = [m[0] for m in matches if m[1] >= 90] \n",
    "    for cm in close_matches:\n",
    "        name_map_global[cm] = name\n",
    "\n",
    "june25_subset['Firm_Final'] = june25_subset['Firm_Final'].map(name_map_global).fillna(june25_subset['Firm_Final'])\n",
    "\n",
    "june25_subset.to_csv('../Data/Cleaned files/june25_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd029e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyxlsb in c:\\users\\19548\\anaconda3\\lib\\site-packages (1.0.10)\n",
      "Found 121 Excel files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19548\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly summary CSV saved!\n"
     ]
    }
   ],
   "source": [
    "!pip install pyxlsb\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "nred_folder = '../Data/NRED/'\n",
    "\n",
    "# Find all Excel files within the folders\n",
    "all_files = glob.glob(os.path.join(nred_folder, '**', '*.xls*'), recursive=True)\n",
    "print(f\"Found {len(all_files)} Excel files\")\n",
    "\n",
    "# Create new list to store\n",
    "month_data = []\n",
    "\n",
    "for file in all_files:\n",
    "    try:\n",
    "        # Read Excel, handle .xlsb and .xls/.xlsx\n",
    "        if file.endswith('.xlsb'):\n",
    "            df = pd.read_excel(file, engine='pyxlsb')\n",
    "        else:\n",
    "            df = pd.read_excel(file)\n",
    "        \n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Try to extract month and year from file name (safer)\n",
    "        # Example file name: \"NRED HOA - 06 2015.xls\"\n",
    "        base_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        parts = base_name.split('-')[-1].strip().split()\n",
    "        if len(parts) != 2:\n",
    "            print(f\"Skipping {file}, cannot extract month/year\")\n",
    "            continue\n",
    "        month = int(parts[0])\n",
    "        year = int(parts[1])\n",
    "\n",
    "        # Filter out condos\n",
    "        df = df[~df['Name'].str.contains('condo', case=False, na=False)]\n",
    "\n",
    "        # Filter zero units using proper column name\n",
    "        df['# of Units'] = pd.to_numeric(df['# of Units'], errors='coerce')\n",
    "        df = df[df['# of Units'] > 0]\n",
    "\n",
    "        # Count total and professionally managed\n",
    "        total_rows = df.shape[0]\n",
    "        professional_rows = (\n",
    "            df['Address1']\n",
    "            .fillna('')\n",
    "            .str.replace(r'\\bC/O\\b', '', regex=True)\n",
    "            .str.strip()\n",
    "            .ne('')\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "        month_data.append({\n",
    "            'Year': year,\n",
    "            'Month': month,\n",
    "            'Total_HOAs': total_rows,\n",
    "            'Professional_HOAs': professional_rows\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "if not month_data:\n",
    "    raise ValueError(\"No valid files found to process!\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "summary_df = pd.DataFrame(month_data)\n",
    "\n",
    "# Sort by Year and Month\n",
    "summary_df = summary_df.sort_values(['Year', 'Month']).reset_index(drop=True)\n",
    "\n",
    "# Save for Stata\n",
    "summary_df.to_csv('../Data/Misc/HOA_summary_monthly_for_stata.csv', index=False)\n",
    "print(\"Monthly summary CSV saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d08a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 121 Excel files\n",
      "Subset written: NRED HOA - 06 2015\n",
      "Subset written: NRED HOA - 07 2015\n",
      "Subset written: NRED HOA - 08 2015\n",
      "Subset written: NRED HOA - 09 2015\n",
      "Subset written: NRED HOA - 10 2015\n",
      "Subset written: NRED HOA - 11 2015\n",
      "Subset written: NRED HOA - 12 2015\n",
      "Subset written: NRED HOA - 01 2016\n",
      "Subset written: NRED HOA - 02 2016\n",
      "Subset written: NRED HOA - 03 2016\n",
      "Subset written: NRED HOA - 04 2016\n",
      "Subset written: NRED HOA - 05 2016\n",
      "Subset written: NRED HOA - 06 2016\n",
      "Subset written: NRED HOA - 07 2016\n",
      "Subset written: NRED HOA - 08 2016\n",
      "Subset written: NRED HOA - 09 2016\n",
      "Subset written: NRED HOA - 10 2016\n",
      "Subset written: NRED HOA - 11 2016\n",
      "Subset written: NRED HOA - 12 2016\n",
      "Subset written: NRED HOA - 01 2017\n",
      "Subset written: NRED HOA - 02 2017\n",
      "Subset written: NRED HOA - 03 2017\n",
      "Subset written: NRED HOA - 04 2017\n",
      "Subset written: NRED HOA - 05 2017\n",
      "Subset written: NRED HOA - 06 2017\n",
      "Subset written: NRED HOA - 07 2017\n",
      "Subset written: NRED HOA - 08 2017\n",
      "Subset written: NRED HOA - 09 2017\n",
      "Subset written: NRED HOA - 10 2017\n",
      "Subset written: NRED HOA - 11 2017\n",
      "Subset written: NRED HOA - 12 2017\n",
      "Subset written: NRED HOA - 01 2018\n",
      "Subset written: NRED HOA - 02 2018\n",
      "Subset written: NRED HOA - 03 2018\n",
      "Subset written: NRED HOA - 04 2018\n",
      "Subset written: NRED HOA - 05 2018\n",
      "Subset written: NRED HOA - 06 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19548\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset written: NRED HOA - 07 2018\n",
      "Subset written: NRED HOA - 08 2018\n",
      "Subset written: NRED HOA - 09 2018\n",
      "Subset written: NRED HOA - 10 2018\n",
      "Subset written: NRED HOA - 11 2018\n",
      "Subset written: NRED HOA - 12 2018\n",
      "Subset written: NRED HOA - 01 2019\n",
      "Subset written: NRED HOA - 02 2019\n",
      "Subset written: NRED HOA - 03 2019\n",
      "Subset written: NRED HOA - 04 2019\n",
      "Subset written: NRED HOA - 05 2019\n",
      "Subset written: NRED HOA - 06 2019\n",
      "Subset written: NRED HOA - 07 2019\n",
      "Subset written: NRED HOA - 08 2019\n",
      "Subset written: NRED HOA - 09 2019\n",
      "Subset written: NRED HOA - 10 2019\n",
      "Subset written: NRED HOA - 11 2019\n",
      "Subset written: NRED HOA - 12 2019\n",
      "Subset written: NRED HOA - 01 2020\n",
      "Subset written: NRED HOA - 02 2020\n",
      "Subset written: NRED HOA - 03 2020\n",
      "Subset written: NRED HOA - 04 2020\n",
      "Subset written: NRED HOA - 05 2020\n",
      "Subset written: NRED HOA - 06 2020\n",
      "Subset written: NRED HOA - 07 2020\n",
      "Subset written: NRED HOA - 08 2020\n",
      "Subset written: NRED HOA - 09 2020\n",
      "Subset written: NRED HOA - 10 2020\n",
      "Subset written: NRED HOA - 11 2020\n",
      "Subset written: NRED HOA - 12 2020\n",
      "Subset written: NRED HOA - 01 2021\n",
      "Subset written: NRED HOA - 02 2021\n",
      "Subset written: NRED HOA - 03 2021\n",
      "Subset written: NRED HOA - 04 2021\n",
      "Subset written: NRED HOA - 05 2021\n",
      "Subset written: NRED HOA - 06 2021\n",
      "Subset written: NRED HOA - 07 2021\n",
      "Subset written: NRED HOA - 08 2021\n",
      "Subset written: NRED HOA - 09 2021\n",
      "Subset written: NRED HOA - 10 2021\n",
      "Subset written: NRED HOA - 11 2021\n",
      "Subset written: NRED HOA - 12 2021\n",
      "Subset written: NRED HOA - 01 2022\n",
      "Subset written: NRED HOA - 02 2022\n",
      "Subset written: NRED HOA - 03 2022\n",
      "Subset written: NRED HOA - 04 2022\n",
      "Subset written: NRED HOA - 05 2022\n",
      "Subset written: NRED HOA - 06 2022\n",
      "Subset written: NRED HOA - 07 2022\n",
      "Subset written: NRED HOA - 08 2022\n",
      "Subset written: NRED HOA - 09 2022\n",
      "Subset written: NRED HOA - 10 2022\n",
      "Subset written: NRED HOA - 11 2022\n",
      "Subset written: NRED HOA - 12 2022\n",
      "Subset written: NRED HOA - 01 2023\n",
      "Subset written: NRED HOA - 02 2023\n",
      "Subset written: NRED HOA - 03 2023\n",
      "Subset written: NRED HOA - 04 2023\n",
      "Subset written: NRED HOA - 05 2023\n",
      "Subset written: NRED HOA - 06 2023\n",
      "Subset written: NRED HOA - 07 2023\n",
      "Subset written: NRED HOA - 08 2023\n",
      "Subset written: NRED HOA - 09 2023\n",
      "Subset written: NRED HOA - 10 2023\n",
      "Subset written: NRED HOA - 11 2023\n",
      "Subset written: NRED HOA - 12 2023\n",
      "Subset written: NRED HOA - 01 2024\n",
      "Subset written: NRED HOA - 02 2024\n",
      "Subset written: NRED HOA - 03 2024\n",
      "Subset written: NRED HOA - 04 2024\n",
      "Subset written: NRED HOA - 05 2024\n",
      "Subset written: NRED HOA - 06 2024\n",
      "Subset written: NRED HOA - 07 2024\n",
      "Subset written: NRED HOA - 08 2024\n",
      "Subset written: NRED HOA - 09 2024\n",
      "Subset written: NRED HOA - 10 2024\n",
      "Subset written: NRED HOA - 11 2024\n",
      "Subset written: NRED HOA - 12 2024\n",
      "Subset written: NRED HOA - 01 2025\n",
      "Subset written: NRED HOA - 02 2025\n",
      "Subset written: NRED HOA - 03 2025\n",
      "Subset written: NRED HOA - 04 2025\n",
      "Subset written: NRED HOA - 05 2025\n",
      "Subset written: NRED HOA - 06 2025\n"
     ]
    }
   ],
   "source": [
    "# Build managed-firm subset ONLY\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "def singularize_word(word):\n",
    "    if len(word) > 3 and word.endswith('s') and not word.endswith('ss'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "\n",
    "def normalize_address(address):\n",
    "    if pd.isnull(address):\n",
    "        return ''\n",
    "    address = address.lower()\n",
    "    address = re.sub(r'\\b(streets|street|st)\\b', 'st', address)\n",
    "    address = re.sub(r'\\b(roads|road|rd)\\b', 'rd', address)\n",
    "    address = re.sub(r'\\b(avenues|avenue|ave)\\b', 'ave', address)\n",
    "    address = re.sub(r'\\b(boulevards|boulevard|blvd)\\b', 'blvd', address)\n",
    "    address = re.sub(r'\\b(suites|suite|ste|se)\\b', 'ste', address)\n",
    "    address = re.sub(r'\\b(apartments|apartment|apt)\\b', 'apt', address)\n",
    "    address = re.sub(r'\\b(forts|fort|ft)\\b', 'ft', address)\n",
    "    address = re.sub(r'\\b(south|s)\\b', 's', address)\n",
    "    address = re.sub(r'\\b(north|n)\\b', 'n', address)\n",
    "    address = re.sub(r'\\b(east|e)\\b', 'e', address)\n",
    "    address = re.sub(r'\\b(west|w)\\b', 'w', address)\n",
    "    address = re.sub(r'\\b(southeast|se)\\b', 'se', address)\n",
    "    address = re.sub(r'\\b(northeast|ne)\\b', 'ne', address)\n",
    "    address = re.sub(r'\\b(southwest|sw)\\b', 'sw', address)\n",
    "    address = re.sub(r'\\b(northwest|nw)\\b', 'nw', address)\n",
    "    address = re.split(r'\\b(suite|ste|apt|apartment|unit|site)\\b', address)[0]\n",
    "    address = re.sub(r'[.,#]', '', address)\n",
    "    address = re.sub(r'\\s+', ' ', address).strip()\n",
    "    address = ' '.join(singularize_word(w) for w in address.split())\n",
    "    return address\n",
    "\n",
    "def normalize_firm_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return ''\n",
    "    name = name.strip().lower()\n",
    "    name = re.sub(r'\\b(llc|inc|corp|corporation|co|company|ltd|limited)\\b', '', name)\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    name = ' '.join(singularize_word(w) for w in name.split())\n",
    "    return name\n",
    "\n",
    "def normalize_phone(phone):\n",
    "    if pd.isnull(phone):\n",
    "        return ''\n",
    "    phone_str = str(phone)\n",
    "    phone_str = re.split(r'[xX]', phone_str)[0]\n",
    "    return re.sub(r'\\D', '', phone_str)\n",
    "\n",
    "def get_mode(series):\n",
    "    mode = series.mode()\n",
    "    return min(mode, key=len) if not mode.empty else series.iloc[0]\n",
    "\n",
    "def pick_canonical(row):\n",
    "    if pd.notnull(row.get('Firm_ByPhone')):\n",
    "        return row['Firm_ByPhone']\n",
    "    elif pd.notnull(row.get('Firm_ByAddress')):\n",
    "        return row['Firm_ByAddress']\n",
    "    return row['Firm_Normalized']\n",
    "\n",
    "root_folder = \"../Data/NRED/\"\n",
    "output_folder = \"../Data/Cleaned files/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "excel_files = glob.glob(os.path.join(root_folder, '**', '*.xls*'), recursive=True)\n",
    "print(f\"Found {len(excel_files)} Excel files\")\n",
    "\n",
    "\n",
    "for file in excel_files:\n",
    "    try:\n",
    "        df = pd.read_excel(file, dtype=str)\n",
    "\n",
    "        for col in ['Name', 'Address1', 'Address2', 'Telephone', 'City']:\n",
    "            if col in df.columns:\n",
    "                df[col] = (\n",
    "                    df[col].fillna('')\n",
    "                    .str.replace('\\xa0', ' ', regex=False)\n",
    "                    .str.strip()\n",
    "                    .str.replace(r'\\s+', ' ', regex=True)\n",
    "                )\n",
    "\n",
    "        df = df[~df['Name'].str.contains('condo', case=False, na=False)]\n",
    "        df = df[df['# of Units'] != '0']\n",
    "        \n",
    "        df['Address1_clean'] = df['Address1'].str.replace(r'\\bC/O\\b', '', regex=True).str.strip()\n",
    "        subset = df[df['Address1_clean'].str.strip() != ''].copy()\n",
    "        \n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        subset['City'] = subset['City'].str.title()\n",
    "        subset['Address1'] = subset['Address1_clean'].str.title()\n",
    "        subset['Address2'] = subset['Address2'].str.title()\n",
    "        subset['Name'] = subset['Name'].str.title()\n",
    "\n",
    "        subset['Address2_Normalized'] = subset['Address2'].apply(normalize_address)\n",
    "        subset['Firm_Normalized'] = subset['Address1'].apply(normalize_firm_name)\n",
    "        subset['Phone_Normalized'] = subset['Telephone'].apply(normalize_phone).replace('', pd.NA)\n",
    "\n",
    "        subset['Firm_Normalized'] = subset['Firm_Normalized'].replace({\n",
    "            \"ams mangement\": \"ams management group\",\n",
    "            \"firstservice residential\": \"firstservice residential nevada\",\n",
    "            \"first service residential\": \"firstservice residential nevada\",\n",
    "            \"level community management\": \"level property management\",\n",
    "            \"ccmc\": \"capital consultant management\",\n",
    "            \"seabeeze management\": \"seabreeze management\",\n",
    "            \"westward 360\": \"westward360\",\n",
    "            \"terra west property management\": \"terra west management service\",\n",
    "            \"nicklin community management\": \"nicklin community management service\",\n",
    "            \"nicklin property management\": \"nicklin community management service\",\n",
    "            \"associa nevada south\": \"associa\",\n",
    "            \"associa sierra north\": \"associa\",\n",
    "            \"nicklin property management investment\": \"nicklin community management service\"\n",
    "        })\n",
    "\n",
    "        canonical_phone = (\n",
    "            subset.groupby('Phone_Normalized')['Firm_Normalized']\n",
    "            .agg(get_mode)\n",
    "            .rename('Firm_ByPhone')\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        canonical_address = (\n",
    "            subset.groupby('Address2_Normalized')['Firm_Normalized']\n",
    "            .agg(get_mode)\n",
    "            .rename('Firm_ByAddress')\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        subset = subset.merge(canonical_phone, on='Phone_Normalized', how='left')\n",
    "        subset = subset.merge(canonical_address, on='Address2_Normalized', how='left')\n",
    "\n",
    "        subset['Firm_Final'] = subset.apply(pick_canonical, axis=1)\n",
    "\n",
    "        firm_frequency = subset['Firm_Normalized'].value_counts()\n",
    "        STABLE_THRESHOLD = 10\n",
    "\n",
    "        subset['Firm_Final'] = subset.apply(\n",
    "            lambda r: r['Firm_Normalized']\n",
    "            if r['Firm_Normalized'] != r['Firm_Final']\n",
    "            and firm_frequency.get(r['Firm_Normalized'], 0) >= STABLE_THRESHOLD\n",
    "            else r['Firm_Final'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        subset['Name_Changed_Flag'] = subset['Firm_Normalized'] != subset['Firm_Final']\n",
    "\n",
    "        base = os.path.splitext(os.path.basename(file))[0]\n",
    "        subset.to_csv(os.path.join(output_folder, f\"{base}_subset.csv\"), index=False)\n",
    "\n",
    "        print(f\"Subset written: {base}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68313be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full file written: NRED HOA - 06 2015\n",
      "Full file written: NRED HOA - 07 2015\n",
      "Full file written: NRED HOA - 08 2015\n",
      "Full file written: NRED HOA - 09 2015\n",
      "Full file written: NRED HOA - 10 2015\n",
      "Full file written: NRED HOA - 11 2015\n",
      "Full file written: NRED HOA - 12 2015\n",
      "Full file written: NRED HOA - 01 2016\n",
      "Full file written: NRED HOA - 02 2016\n",
      "Full file written: NRED HOA - 03 2016\n",
      "Full file written: NRED HOA - 04 2016\n",
      "Full file written: NRED HOA - 05 2016\n",
      "Full file written: NRED HOA - 06 2016\n",
      "Full file written: NRED HOA - 07 2016\n",
      "Full file written: NRED HOA - 08 2016\n",
      "Full file written: NRED HOA - 09 2016\n",
      "Full file written: NRED HOA - 10 2016\n",
      "Full file written: NRED HOA - 11 2016\n",
      "Full file written: NRED HOA - 12 2016\n",
      "Full file written: NRED HOA - 01 2017\n",
      "Full file written: NRED HOA - 02 2017\n",
      "Full file written: NRED HOA - 03 2017\n",
      "Full file written: NRED HOA - 04 2017\n",
      "Full file written: NRED HOA - 05 2017\n",
      "Full file written: NRED HOA - 06 2017\n",
      "Full file written: NRED HOA - 07 2017\n",
      "Full file written: NRED HOA - 08 2017\n",
      "Full file written: NRED HOA - 09 2017\n",
      "Full file written: NRED HOA - 10 2017\n",
      "Full file written: NRED HOA - 11 2017\n",
      "Full file written: NRED HOA - 12 2017\n",
      "Full file written: NRED HOA - 01 2018\n",
      "Full file written: NRED HOA - 02 2018\n",
      "Full file written: NRED HOA - 03 2018\n",
      "Full file written: NRED HOA - 04 2018\n",
      "Full file written: NRED HOA - 05 2018\n",
      "Full file written: NRED HOA - 06 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19548\\anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full file written: NRED HOA - 07 2018\n",
      "Full file written: NRED HOA - 08 2018\n",
      "Full file written: NRED HOA - 09 2018\n",
      "Full file written: NRED HOA - 10 2018\n",
      "Full file written: NRED HOA - 11 2018\n",
      "Full file written: NRED HOA - 12 2018\n",
      "Full file written: NRED HOA - 01 2019\n",
      "Full file written: NRED HOA - 02 2019\n",
      "Full file written: NRED HOA - 03 2019\n",
      "Full file written: NRED HOA - 04 2019\n",
      "Full file written: NRED HOA - 05 2019\n",
      "Full file written: NRED HOA - 06 2019\n",
      "Full file written: NRED HOA - 07 2019\n",
      "Full file written: NRED HOA - 08 2019\n",
      "Full file written: NRED HOA - 09 2019\n",
      "Full file written: NRED HOA - 10 2019\n",
      "Full file written: NRED HOA - 11 2019\n",
      "Full file written: NRED HOA - 12 2019\n",
      "Full file written: NRED HOA - 01 2020\n",
      "Full file written: NRED HOA - 02 2020\n",
      "Full file written: NRED HOA - 03 2020\n",
      "Full file written: NRED HOA - 04 2020\n",
      "Full file written: NRED HOA - 05 2020\n",
      "Full file written: NRED HOA - 06 2020\n",
      "Full file written: NRED HOA - 07 2020\n",
      "Full file written: NRED HOA - 08 2020\n",
      "Full file written: NRED HOA - 09 2020\n",
      "Full file written: NRED HOA - 10 2020\n",
      "Full file written: NRED HOA - 11 2020\n",
      "Full file written: NRED HOA - 12 2020\n",
      "Full file written: NRED HOA - 01 2021\n",
      "Full file written: NRED HOA - 02 2021\n",
      "Full file written: NRED HOA - 03 2021\n",
      "Full file written: NRED HOA - 04 2021\n",
      "Full file written: NRED HOA - 05 2021\n",
      "Full file written: NRED HOA - 06 2021\n",
      "Full file written: NRED HOA - 07 2021\n",
      "Full file written: NRED HOA - 08 2021\n",
      "Full file written: NRED HOA - 09 2021\n",
      "Full file written: NRED HOA - 10 2021\n",
      "Full file written: NRED HOA - 11 2021\n",
      "Full file written: NRED HOA - 12 2021\n",
      "Full file written: NRED HOA - 01 2022\n",
      "Full file written: NRED HOA - 02 2022\n",
      "Full file written: NRED HOA - 03 2022\n",
      "Full file written: NRED HOA - 04 2022\n",
      "Full file written: NRED HOA - 05 2022\n",
      "Full file written: NRED HOA - 06 2022\n",
      "Full file written: NRED HOA - 07 2022\n",
      "Full file written: NRED HOA - 08 2022\n",
      "Full file written: NRED HOA - 09 2022\n",
      "Full file written: NRED HOA - 10 2022\n",
      "Full file written: NRED HOA - 11 2022\n",
      "Full file written: NRED HOA - 12 2022\n",
      "Full file written: NRED HOA - 01 2023\n",
      "Full file written: NRED HOA - 02 2023\n",
      "Full file written: NRED HOA - 03 2023\n",
      "Full file written: NRED HOA - 04 2023\n",
      "Full file written: NRED HOA - 05 2023\n",
      "Full file written: NRED HOA - 06 2023\n",
      "Full file written: NRED HOA - 07 2023\n",
      "Full file written: NRED HOA - 08 2023\n",
      "Full file written: NRED HOA - 09 2023\n",
      "Full file written: NRED HOA - 10 2023\n",
      "Full file written: NRED HOA - 11 2023\n",
      "Full file written: NRED HOA - 12 2023\n",
      "Full file written: NRED HOA - 01 2024\n",
      "Full file written: NRED HOA - 02 2024\n",
      "Full file written: NRED HOA - 03 2024\n",
      "Full file written: NRED HOA - 04 2024\n",
      "Full file written: NRED HOA - 05 2024\n",
      "Full file written: NRED HOA - 06 2024\n",
      "Full file written: NRED HOA - 07 2024\n",
      "Full file written: NRED HOA - 08 2024\n",
      "Full file written: NRED HOA - 09 2024\n",
      "Full file written: NRED HOA - 10 2024\n",
      "Full file written: NRED HOA - 11 2024\n",
      "Full file written: NRED HOA - 12 2024\n",
      "Full file written: NRED HOA - 01 2025\n",
      "Full file written: NRED HOA - 02 2025\n",
      "Full file written: NRED HOA - 03 2025\n",
      "Full file written: NRED HOA - 04 2025\n",
      "Full file written: NRED HOA - 05 2025\n",
      "Full file written: NRED HOA - 06 2025\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 â€” Build FULL file by appending unmanaged rows\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "root_folder = \"../Data/NRED/\"\n",
    "cleaned_folder = \"../Data/Cleaned files/\"\n",
    "\n",
    "excel_files = glob.glob(os.path.join(root_folder, '**', '*.xls*'), recursive=True)\n",
    "\n",
    "for file in excel_files:\n",
    "    try:\n",
    "        base = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "        df_raw = pd.read_excel(file, dtype=str)\n",
    "\n",
    "        for col in ['Name', 'Address1', 'Address2', 'Telephone', 'City']:\n",
    "            if col in df_raw.columns:\n",
    "                df_raw[col] = (\n",
    "                    df_raw[col].fillna('')\n",
    "                    .str.replace('\\xa0', ' ', regex=False)\n",
    "                    .str.strip()\n",
    "                    .str.replace(r'\\s+', ' ', regex=True)\n",
    "                )\n",
    "\n",
    "        unmanaged = df_raw[df_raw['Address1'].str.strip() == ''].copy()\n",
    "\n",
    "        subset_path = os.path.join(cleaned_folder, f\"{base}_subset.csv\")\n",
    "        if not os.path.exists(subset_path):\n",
    "            continue\n",
    "\n",
    "        subset = pd.read_csv(subset_path, dtype=str)\n",
    "\n",
    "        for col in subset.columns:\n",
    "            if col not in unmanaged.columns:\n",
    "                unmanaged[col] = pd.NA\n",
    "\n",
    "        for col in unmanaged.columns:\n",
    "            if col not in subset.columns:\n",
    "                subset[col] = pd.NA\n",
    "\n",
    "        full_df = pd.concat(\n",
    "            [subset, unmanaged],\n",
    "            ignore_index=True,\n",
    "            sort=False\n",
    "        )\n",
    "\n",
    "        full_df.to_csv(\n",
    "            os.path.join(cleaned_folder, f\"{base}_full_plus_subset.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        print(f\"Full file written: {base}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ce7355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 December CSV files\n",
      "Wide HOA dataset saved to: ../Data/Cleaned files/HOA_Firm_Wide_Decembers.csv\n",
      "Shape: (3643, 23)\n",
      "\n",
      "Diagnostic: December snapshot vs wide panel\n",
      "   Year  December_HOAs  Wide_panel_rows\n",
      "0  2015           3070             3643\n",
      "1  2016           3129             3643\n",
      "2  2017           3168             3643\n",
      "3  2018           3265             3643\n",
      "4  2019           3346             3643\n",
      "5  2020           3386             3643\n",
      "6  2021           3395             3643\n",
      "7  2022           3541             3643\n",
      "8  2023           3608             3643\n",
      "9  2024           3682             3643\n",
      "\n",
      "Wide panel growth year by year:\n",
      "   Year  December_HOAs  Wide_panel_rows_so_far\n",
      "0  2015           3070                    3070\n",
      "1  2016           3129                    3153\n",
      "2  2017           3168                    3211\n",
      "3  2018           3265                    3329\n",
      "4  2019           3346                    3441\n",
      "5  2020           3386                    3529\n",
      "6  2021           3395                    3633\n",
      "7  2022           3541                    3719\n",
      "8  2023           3608                    3796\n",
      "9  2024           3682                    3881\n",
      "\n",
      "Percentage of frequent changers in the wide dataset: 33.46% (1219 of 3643 rows)\n",
      "Number of HOAs with a listed firm in every December 2015-2024: 2489 of 3643 rows\n"
     ]
    }
   ],
   "source": [
    "# Build wide version\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "input_folder = \"../Data/Cleaned files/\"\n",
    "output_file = os.path.join(input_folder, \"HOA_Firm_Wide_Decembers.csv\")\n",
    "\n",
    "all_full_files = glob.glob(os.path.join(input_folder, \"*_full_plus_subset.csv\"))\n",
    "\n",
    "csv_files = [f for f in all_full_files if re.search(r'- 12 ', os.path.basename(f))]\n",
    "print(f\"Found {len(csv_files)} December CSV files\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    base_name = os.path.basename(file)\n",
    "    \n",
    "    year_search = re.search(r'(\\d{4})', base_name)\n",
    "    if year_search:\n",
    "        year = int(year_search.group(1))\n",
    "    else:\n",
    "        print(f\"Warning: Could not detect year from filename {base_name}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    \n",
    "    df = df[['Name', 'Firm_Final']].copy()\n",
    "    \n",
    "    df['Exists'] = 1\n",
    "\n",
    "    df['Name'] = df['Name'].str.strip().str.title()\n",
    "    \n",
    "    df['Year'] = year\n",
    "    \n",
    "    all_dfs.append(df)\n",
    "\n",
    "long_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "wide_firm = long_df.pivot_table(\n",
    "    index='Name',\n",
    "    columns='Year',\n",
    "    values='Firm_Final',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "wide_exists = long_df.pivot_table(\n",
    "    index='Name',\n",
    "    columns='Year',\n",
    "    values='Exists',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "wide_firm.columns = wide_firm.columns.map(str)\n",
    "wide_exists.columns = wide_exists.columns.map(str)\n",
    "\n",
    "wide_firm = wide_firm.reset_index()\n",
    "wide_exists = wide_exists.reset_index()\n",
    "\n",
    "wide_df_complete = wide_firm.merge(\n",
    "    wide_exists,\n",
    "    on='Name',\n",
    "    suffixes=('', '_exists')\n",
    ")\n",
    "\n",
    "year_columns = sorted(\n",
    "    [col for col in wide_firm.columns if col != 'Name']\n",
    ")\n",
    "\n",
    "def count_changes(row):\n",
    "    firms = [row[year] for year in year_columns]\n",
    "    changes = 0\n",
    "    for f1, f2 in zip(firms[:-1], firms[1:]):\n",
    "        # Count change if values differ\n",
    "        # AND at least one period has a non-missing firm\n",
    "        if f1 != f2 and (pd.notna(f1) or pd.notna(f2)):\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "wide_df_complete['Number of changes'] = wide_df_complete.apply(\n",
    "    count_changes, axis=1\n",
    ")\n",
    "\n",
    "wide_df_complete['Frequent changer'] = (\n",
    "    wide_df_complete['Number of changes'] >= 2\n",
    ")\n",
    "\n",
    "wide_df_complete.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Wide HOA dataset saved to: {output_file}\")\n",
    "print(\"Shape:\", wide_df_complete.shape)\n",
    "\n",
    "dec_counts = long_df.groupby('Year')['Name'].nunique().reset_index()\n",
    "dec_counts.columns = ['Year', 'December_HOAs']\n",
    "dec_counts['Wide_panel_rows'] = wide_df_complete.shape[0]\n",
    "\n",
    "print(\"\\nDiagnostic: December snapshot vs wide panel\")\n",
    "print(dec_counts)\n",
    "\n",
    "# -----------------------------\n",
    "# Year-by-year wide panel growth\n",
    "# -----------------------------\n",
    "years_sorted = sorted(long_df['Year'].unique())\n",
    "rows_over_time = []\n",
    "\n",
    "current_hoas = set()\n",
    "\n",
    "for year in years_sorted:\n",
    "    # HOAs in this December\n",
    "    dec_hoas = set(long_df.loc[long_df['Year'] == year, 'Name'])\n",
    "    \n",
    "    # Add to the running set of all HOAs included so far\n",
    "    current_hoas.update(dec_hoas)\n",
    "    \n",
    "    rows_over_time.append({\n",
    "        'Year': year,\n",
    "        'December_HOAs': len(dec_hoas),\n",
    "        'Wide_panel_rows_so_far': len(current_hoas)\n",
    "    })\n",
    "\n",
    "wide_growth_df = pd.DataFrame(rows_over_time)\n",
    "\n",
    "print(\"\\nWide panel growth year by year:\")\n",
    "print(wide_growth_df)\n",
    "\n",
    "####### Summary stats\n",
    "num_rows = wide_df_complete.shape[0]\n",
    "num_frequent = wide_df_complete['Frequent changer'].sum()\n",
    "pct_frequent = 100 * num_frequent / num_rows\n",
    "\n",
    "print(f\"\\nPercentage of frequent changers in the wide dataset: {pct_frequent:.2f}% ({num_frequent} of {num_rows} rows)\")\n",
    "\n",
    "year_columns = [str(y) for y in range(2015, 2025)]\n",
    "rows_all_firm = wide_df_complete[year_columns].notna().all(axis=1).sum()\n",
    "\n",
    "print(f\"Number of HOAs with a listed firm in every December 2015-2024: {rows_all_firm} of {num_rows} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
